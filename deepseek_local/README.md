# DeepSeek Chat - Local AI

This project leverages **DeepSeek** models with **Streamlit** to create a 100% locally running AI-powered chat application.

---

## ğŸš€ Installation and Setup

### **1ï¸âƒ£ Install Ollama**
Ollama is required to run DeepSeek models locally. Install it using:

#### **For Linux & Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```
#### **For Windows:**
Download and install Ollama from the official website:  
ğŸ”— [Ollama Official Site](https://ollama.com/)

Once installed, **pull the DeepSeek model**:
```bash
ollama pull deepseek-chat
```

---

### **2ï¸âƒ£ Install Dependencies**
Ensure you have **Python 3.11+** installed. Then, install the required Python libraries:

```bash
pip install streamlit ollama
```

---

### **3ï¸âƒ£ Run the Application**
To start the chat application, run:

```bash
streamlit run app.py
```

This will launch the **DeepSeek Chat** app in your browser.

---

## ğŸ› ï¸ Troubleshooting

### **Q: The model isn't responding. What do I do?**  
ğŸ”¹ Ensure **Ollama** is running (`ollama list` should show available models).  
ğŸ”¹ Try restarting Ollama:  
```bash
ollama stop
ollama run deepseek-chat
```

### **Q: I get `ModuleNotFoundError` when running the script.**  
ğŸ”¹ Install missing dependencies using:  
```bash
pip install -r requirements.txt
```

### **Q: How do I reset the chat sessions?**  
ğŸ”¹ Delete the `chat_sessions.json` file in your project directory.

---


